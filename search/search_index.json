{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AI-powered search engine txtai executes machine-learning workflows to transform data and build AI-powered text indices to perform similarity search. Summary of txtai features: \ud83d\udd0e Large-scale similarity search with multiple index backends ( Faiss , Annoy , Hnswlib ) \ud83d\udcc4 Create embeddings for text snippets, documents, audio and images. Supports transformers and word vectors. \ud83d\udca1 Machine-learning pipelines to run extractive question-answering, zero-shot labeling, transcription, translation, summarization and text extraction \u21aa\ufe0f\ufe0f Workflows that join pipelines together to aggregate business logic. txtai processes can be microservices or full-fledged indexing workflows. \ud83d\udd17 API bindings for JavaScript , Java , Rust and Go \u2601\ufe0f Cloud-native architecture that scales out with container orchestration systems (e.g. Kubernetes) NeuML uses txtai and/or the concepts behind it to power all of our Natural Language Processing (NLP) applications. Applications include: Application Description paperai AI-powered literature discovery and review engine for medical/scientific papers tldrstory AI-powered understanding of headlines and story text neuspo Fact-driven, real-time sports event and news site codequestion Ask coding questions directly from the terminal txtai is built with Python 3.6+, Hugging Face Transformers , Sentence Transformers and FastAPI","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"api/","text":"API txtai has a full-featured API that can optionally be enabled for any txtai process. All functionality found in txtai can be accessed via the API. The following is an example configuration and startup script for the API. Note that this configuration file enables all functionality (embeddings, extractor, labels, similarity). It is suggested that separate processes are used for each instance of a txtai component. # Index file path path : /tmp/index # Allow indexing of documents writable : True # Embeddings settings embeddings : method : transformers path : sentence-transformers/bert-base-nli-mean-tokens # Extractor settings extractor : path : distilbert-base-cased-distilled-squad # Labels settings labels : # Similarity settings similarity : Assuming this YAML content is stored in a file named index.yml, the following command starts the API process. CONFIG=index.yml uvicorn \"txtai.api:app\" uvicorn is a full-featured production ready server with support for SSL and more. See the uvicorn deployment guide for details. Docker A Dockerfile with commands to install txtai, all dependencies and default configuration is available in this repository. The Dockerfile can be copied from the docker directory on GitHub locally. The following commands show how to run the API process. docker build -t txtai.api -f docker/api.Dockerfile . docker run --name txtai.api -p 8000 :8000 --rm -it txtai.api # Alternatively, if nvidia-docker is installed, the build will support GPU runtimes docker run --name txtai.api --runtime = nvidia -p 8000 :8000 --rm -it txtai.api This will bring up an API instance without having to install Python, txtai or any dependencies on your machine! Differences between Python and API The txtai API provides all the major functionality found in this project. But there are differences due to the nature of JSON and differences across the supported programming languages. Difference Python API Reason Return Types tuples objects Consistency across languages. For example, (id, score) in Python is {\"id\": value, \"score\": value} via API Extractor extract() extractor.extract() Extractor pipeline is a callable object in Python Labels labels() labels.label() Labels pipeline is a callable object in Python that supports both string and list input Similarity similarity() similarity.similarity() Similarity pipeline a callable object in Python that supports both string and list input Supported language bindings The following programming languages have txtai bindings: JavaScript Java Rust Go See each of the projects above for details on how to install and use. Please add an issue to request additional language bindings! application FastAPI application module get () Returns global API instance. Returns: Type Description API instance Source code in txtai/api/application.py def get (): \"\"\" Returns global API instance. Returns: API instance \"\"\" # pylint: disable=W0603 global INSTANCE return INSTANCE start () FastAPI startup event. Loads API instance. Source code in txtai/api/application.py @app . on_event ( \"startup\" ) def start (): \"\"\" FastAPI startup event. Loads API instance. \"\"\" # pylint: disable=W0603 global INSTANCE # Load YAML settings with open ( os . getenv ( \"CONFIG\" ), \"r\" ) as f : # Read configuration config = yaml . safe_load ( f ) # Instantiate API instance api = os . getenv ( \"API_CLASS\" ) INSTANCE = Factory . create ( config , api ) if api else API ( config ) # Router definitions routers = [ ( \"embeddings\" , embeddings . router ), ( \"extractor\" , extractor . router ), ( \"labels\" , labels . router ), ( \"similarity\" , similarity . router ), ( \"summary\" , summary . router ), ( \"textractor\" , textractor . router ), ( \"transcription\" , transcription . router ), ( \"translation\" , translation . router ), ] # Conditionally add routes based on configuration for route , router in routers : if route in config : app . include_router ( router ) # Special case to add similarity instance for embeddings if \"embeddings\" in config and \"similarity\" not in config : app . include_router ( similarity . router ) # Execute extensions if present extensions = os . getenv ( \"EXTENSIONS\" ) if extensions : for extension in extensions . split ( \",\" ): # Create instance and execute extension extension = Factory . get ( extension . strip ())() extension ( app )","title":"API"},{"location":"api/#api","text":"txtai has a full-featured API that can optionally be enabled for any txtai process. All functionality found in txtai can be accessed via the API. The following is an example configuration and startup script for the API. Note that this configuration file enables all functionality (embeddings, extractor, labels, similarity). It is suggested that separate processes are used for each instance of a txtai component. # Index file path path : /tmp/index # Allow indexing of documents writable : True # Embeddings settings embeddings : method : transformers path : sentence-transformers/bert-base-nli-mean-tokens # Extractor settings extractor : path : distilbert-base-cased-distilled-squad # Labels settings labels : # Similarity settings similarity : Assuming this YAML content is stored in a file named index.yml, the following command starts the API process. CONFIG=index.yml uvicorn \"txtai.api:app\" uvicorn is a full-featured production ready server with support for SSL and more. See the uvicorn deployment guide for details.","title":"API"},{"location":"api/#docker","text":"A Dockerfile with commands to install txtai, all dependencies and default configuration is available in this repository. The Dockerfile can be copied from the docker directory on GitHub locally. The following commands show how to run the API process. docker build -t txtai.api -f docker/api.Dockerfile . docker run --name txtai.api -p 8000 :8000 --rm -it txtai.api # Alternatively, if nvidia-docker is installed, the build will support GPU runtimes docker run --name txtai.api --runtime = nvidia -p 8000 :8000 --rm -it txtai.api This will bring up an API instance without having to install Python, txtai or any dependencies on your machine!","title":"Docker"},{"location":"api/#differences-between-python-and-api","text":"The txtai API provides all the major functionality found in this project. But there are differences due to the nature of JSON and differences across the supported programming languages. Difference Python API Reason Return Types tuples objects Consistency across languages. For example, (id, score) in Python is {\"id\": value, \"score\": value} via API Extractor extract() extractor.extract() Extractor pipeline is a callable object in Python Labels labels() labels.label() Labels pipeline is a callable object in Python that supports both string and list input Similarity similarity() similarity.similarity() Similarity pipeline a callable object in Python that supports both string and list input","title":"Differences between Python and API"},{"location":"api/#supported-language-bindings","text":"The following programming languages have txtai bindings: JavaScript Java Rust Go See each of the projects above for details on how to install and use. Please add an issue to request additional language bindings!","title":"Supported language bindings"},{"location":"api/#txtai.api.application","text":"FastAPI application module","title":"application"},{"location":"api/#txtai.api.application.get","text":"Returns global API instance. Returns: Type Description API instance Source code in txtai/api/application.py def get (): \"\"\" Returns global API instance. Returns: API instance \"\"\" # pylint: disable=W0603 global INSTANCE return INSTANCE","title":"get()"},{"location":"api/#txtai.api.application.start","text":"FastAPI startup event. Loads API instance. Source code in txtai/api/application.py @app . on_event ( \"startup\" ) def start (): \"\"\" FastAPI startup event. Loads API instance. \"\"\" # pylint: disable=W0603 global INSTANCE # Load YAML settings with open ( os . getenv ( \"CONFIG\" ), \"r\" ) as f : # Read configuration config = yaml . safe_load ( f ) # Instantiate API instance api = os . getenv ( \"API_CLASS\" ) INSTANCE = Factory . create ( config , api ) if api else API ( config ) # Router definitions routers = [ ( \"embeddings\" , embeddings . router ), ( \"extractor\" , extractor . router ), ( \"labels\" , labels . router ), ( \"similarity\" , similarity . router ), ( \"summary\" , summary . router ), ( \"textractor\" , textractor . router ), ( \"transcription\" , transcription . router ), ( \"translation\" , translation . router ), ] # Conditionally add routes based on configuration for route , router in routers : if route in config : app . include_router ( router ) # Special case to add similarity instance for embeddings if \"embeddings\" in config and \"similarity\" not in config : app . include_router ( similarity . router ) # Execute extensions if present extensions = os . getenv ( \"EXTENSIONS\" ) if extensions : for extension in extensions . split ( \",\" ): # Create instance and execute extension extension = Factory . get ( extension . strip ())() extension ( app )","title":"start()"},{"location":"embeddings/","text":"Embeddings An Embeddings instance is the engine that provides similarity search. Embeddings can be used to run ad-hoc similarity comparisions or build/search large indices. Embeddings parameters are set through the constructor. Examples below. # Transformers embeddings model Embeddings ({ \"method\" : \"transformers\" , \"path\" : \"sentence-transformers/bert-base-nli-mean-tokens\" }) # Word embeddings model Embeddings ({ \"path\" : vectors , \"storevectors\" : True , \"scoring\" : \"bm25\" , \"pca\" : 3 , \"quantize\" : True }) Configuration method method : transformers|words Sets the sentence embeddings method to use. When set to transformers , the embeddings object builds sentence embeddings using the sentence transformers. Otherwise a word embeddings model is used. Defaults to words. path path : string Required field that sets the path for a vectors model. When method set to transformers , this must be a path to a Hugging Face transformers model. Otherwise, it must be a path to a local word embeddings model. tokenize tokenize : boolean Enables string tokenization (defaults to true). This method applies tokenization rules that work best with English language text and help increase the quality of English language sentence embeddings. This should be disabled when working with non-English text. storevectors storevectors : boolean Enables copying of a vectors model set in path into the embeddings models output directory on save. This option enables a fully encapsulated index with no external file dependencies. scoring scoring : bm25|tfidf|sif For word embedding models, a scoring model allows building weighted averages of word vectors for a given sentence. Supports BM25, tf-idf and SIF (smooth inverse frequency) methods. If a scoring method is not provided, mean sentence embeddings are built. pca pca : int Removes n principal components from generated sentence embeddings. When enabled, a TruncatedSVD model is built to help with dimensionality reduction. After pooling of vectors creates a single sentence embedding, this method is applied. backend backend : annoy|faiss|hnsw Approximate Nearest Neighbor (ANN) index backend for storing generated sentence embeddings. Defaults to Faiss for Linux/macOS and Annoy for Windows. Faiss currently is not supported on Windows. Backend-specific settings are set with a corresponding configuration object having the same name as the backend (i.e. annoy, faiss, or hnsw). None of these are required and are set to defaults if omitted. annoy annoy : ntrees : number of trees (int) - defaults to 10 searchk : search_k search setting (int) - defaults to -1 See Annoy documentation for more information on these parameters. faiss faiss : components : Comma separated list of components - defaults to None nprobe : search probe setting (int) - defaults to 6 See Faiss documentation on the index factory and search for more information on these parameters. hnsw hnsw : efconstruction : ef_construction param for init_index (int) - defaults to 200 m : M param for init_index (int) - defaults to 16 randomseed : random-seed param for init_index (init) - defaults to 100 efsearch : ef search param (int) - defaults to None and not set See Hnswlib documentation for more information on these parameters. quantize quantize : boolean Enables quanitization of generated sentence embeddings. If the index backend supports it, sentence embeddings will be stored with 8-bit precision vs 32-bit. Only Faiss currently supports quantization. __init__ ( self , config = None ) special Creates a new Embeddings model. Parameters: Name Type Description Default config embeddings configuration None Source code in txtai/embeddings/base.py def __init__ ( self , config = None ): \"\"\" Creates a new Embeddings model. Args: config: embeddings configuration \"\"\" # Configuration self . config = config # Embeddings model self . embeddings = None # Dimensionality reduction model self . lsa = None # Embedding scoring method - weighs each word in a sentence self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) if self . config and self . config . get ( \"scoring\" ) else None # Sentence vectors model self . model = self . loadVectors () if self . config else None batchsearch ( self , queries , limit = 3 ) Finds documents in the embeddings model most similar to the input queries. Returns a list of (id, score) sorted by highest score per query, where id is the document id in the embeddings model. Parameters: Name Type Description Default queries queries text|tokens required limit maximum results 3 Returns: Type Description list of (id, score) per query Source code in txtai/embeddings/base.py def batchsearch ( self , queries , limit = 3 ): \"\"\" Finds documents in the embeddings model most similar to the input queries. Returns a list of (id, score) sorted by highest score per query, where id is the document id in the embeddings model. Args: queries: queries text|tokens limit: maximum results Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors embeddings = np . array ([ self . transform (( None , query , None )) for query in queries ]) # Search embeddings index results = self . embeddings . search ( embeddings , limit ) # Map ids if id mapping available lookup = self . config . get ( \"ids\" ) if lookup : results = [[( lookup [ i ], score ) for i , score in r ] for r in results ] return results batchsimilarity ( self , queries , texts ) Computes the similarity between list of queries and list of text. Returns a list of (id, score) sorted by highest score per query, where id is the index in texts. Parameters: Name Type Description Default queries queries text|tokens required texts list of text|tokens required Returns: Type Description list of (id, score) per query Source code in txtai/embeddings/base.py def batchsimilarity ( self , queries , texts ): \"\"\" Computes the similarity between list of queries and list of text. Returns a list of (id, score) sorted by highest score per query, where id is the index in texts. Args: queries: queries text|tokens texts: list of text|tokens Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors queries = np . array ([ self . transform (( None , query , None )) for query in queries ]) texts = np . array ([ self . transform (( None , text , None )) for text in texts ]) # Dot product on normalized vectors is equal to cosine similarity scores = np . dot ( queries , texts . T ) . tolist () # Add index id and sort desc based on score return [ sorted ( enumerate ( score ), key = lambda x : x [ 1 ], reverse = True ) for score in scores ] batchtransform ( self , documents ) Transforms documents into embeddings vectors. Document text will be tokenized if not pre-tokenized. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Returns: Type Description embeddings vectors Source code in txtai/embeddings/base.py def batchtransform ( self , documents ): \"\"\" Transforms documents into embeddings vectors. Document text will be tokenized if not pre-tokenized. Args: documents: list of (id, text|tokens, tags) Returns: embeddings vectors \"\"\" return [ self . transform ( document ) for document in documents ] index ( self , documents ) Builds an embeddings index. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Source code in txtai/embeddings/base.py def index ( self , documents ): \"\"\" Builds an embeddings index. Args: documents: list of (id, text|tokens, tags) \"\"\" # Transform documents to embeddings vectors ids , dimensions , stream = self . model . index ( documents ) # Load streamed embeddings back to memory embeddings = np . empty (( len ( ids ), dimensions ), dtype = np . float32 ) with open ( stream , \"rb\" ) as queue : for x in range ( embeddings . shape [ 0 ]): embeddings [ x ] = pickle . load ( queue ) # Remove temporary file os . remove ( stream ) # Build LSA model (if enabled). Remove principal components from embeddings. if self . config . get ( \"pca\" ): self . lsa = self . buildLSA ( embeddings , self . config [ \"pca\" ]) self . removePC ( embeddings ) # Normalize embeddings self . normalize ( embeddings ) # Save embeddings metadata self . config [ \"ids\" ] = ids self . config [ \"dimensions\" ] = dimensions # Create embeddings index self . embeddings = ANNFactory . create ( self . config ) # Build the index self . embeddings . index ( embeddings ) load ( self , path ) Loads a pre-trained model. Models have the following files: config - configuration embeddings - sentence embeddings index lsa - LSA model, used to remove the principal component(s) scoring - scoring model used to weigh word vectors vectors - vectors model Parameters: Name Type Description Default path input directory path required Source code in txtai/embeddings/base.py def load ( self , path ): \"\"\" Loads a pre-trained model. Models have the following files: config - configuration embeddings - sentence embeddings index lsa - LSA model, used to remove the principal component(s) scoring - scoring model used to weigh word vectors vectors - vectors model Args: path: input directory path \"\"\" # Index configuration with open ( \" %s /config\" % path , \"rb\" ) as handle : self . config = pickle . load ( handle ) # Build full path to embedding vectors file if self . config . get ( \"storevectors\" ): self . config [ \"path\" ] = os . path . join ( path , self . config [ \"path\" ]) # Sentence embeddings index self . embeddings = ANNFactory . create ( self . config ) self . embeddings . load ( \" %s /embeddings\" % path ) # Dimensionality reduction if self . config . get ( \"pca\" ): with open ( \" %s /lsa\" % path , \"rb\" ) as handle : self . lsa = pickle . load ( handle ) # Embedding scoring if self . config . get ( \"scoring\" ): self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) self . scoring . load ( path ) # Sentence vectors model - transforms text into sentence embeddings self . model = self . loadVectors () save ( self , path ) Saves a model. Parameters: Name Type Description Default path output directory path required Source code in txtai/embeddings/base.py def save ( self , path ): \"\"\" Saves a model. Args: path: output directory path \"\"\" if self . config : # Create output directory, if necessary os . makedirs ( path , exist_ok = True ) # Copy vectors file if self . config . get ( \"storevectors\" ): shutil . copyfile ( self . config [ \"path\" ], os . path . join ( path , os . path . basename ( self . config [ \"path\" ]))) self . config [ \"path\" ] = os . path . basename ( self . config [ \"path\" ]) # Write index configuration with open ( \" %s /config\" % path , \"wb\" ) as handle : pickle . dump ( self . config , handle , protocol = pickle . HIGHEST_PROTOCOL ) # Write sentence embeddings index self . embeddings . save ( \" %s /embeddings\" % path ) # Save dimensionality reduction if self . lsa : with open ( \" %s /lsa\" % path , \"wb\" ) as handle : pickle . dump ( self . lsa , handle , protocol = pickle . HIGHEST_PROTOCOL ) # Save embedding scoring if self . scoring : self . scoring . save ( path ) score ( self , documents ) Builds a scoring index. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Source code in txtai/embeddings/base.py def score ( self , documents ): \"\"\" Builds a scoring index. Args: documents: list of (id, text|tokens, tags) \"\"\" if self . scoring : # Build scoring index over documents self . scoring . index ( documents ) search ( self , query , limit = 3 ) Finds documents in the embeddings model most similar to the input query. Returns a list of (id, score) sorted by highest score, where id is the document id in the embeddings model. Parameters: Name Type Description Default query query text|tokens required limit maximum results 3 Returns: Type Description list of (id, score) Source code in txtai/embeddings/base.py def search ( self , query , limit = 3 ): \"\"\" Finds documents in the embeddings model most similar to the input query. Returns a list of (id, score) sorted by highest score, where id is the document id in the embeddings model. Args: query: query text|tokens limit: maximum results Returns: list of (id, score) \"\"\" return self . batchsearch ([ query ], limit )[ 0 ] similarity ( self , query , texts ) Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. Parameters: Name Type Description Default query query text|tokens required texts list of text|tokens required Returns: Type Description list of (id, score) Source code in txtai/embeddings/base.py def similarity ( self , query , texts ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. Args: query: query text|tokens texts: list of text|tokens Returns: list of (id, score) \"\"\" return self . batchsimilarity ([ query ], texts )[ 0 ] transform ( self , document ) Transforms document into an embeddings vector. Document text will be tokenized if not pre-tokenized. Parameters: Name Type Description Default document (id, text|tokens, tags) required Returns: Type Description embeddings vector Source code in txtai/embeddings/base.py def transform ( self , document ): \"\"\" Transforms document into an embeddings vector. Document text will be tokenized if not pre-tokenized. Args: document: (id, text|tokens, tags) Returns: embeddings vector \"\"\" # Convert document into sentence embedding embedding = self . model . transform ( document ) # Reduce the dimensionality of the embeddings. Scale the embeddings using this # model to reduce the noise of common but less relevant terms. if self . lsa : self . removePC ( embedding ) # Normalize embeddings self . normalize ( embedding ) return embedding","title":"Embeddings"},{"location":"embeddings/#embeddings","text":"An Embeddings instance is the engine that provides similarity search. Embeddings can be used to run ad-hoc similarity comparisions or build/search large indices. Embeddings parameters are set through the constructor. Examples below. # Transformers embeddings model Embeddings ({ \"method\" : \"transformers\" , \"path\" : \"sentence-transformers/bert-base-nli-mean-tokens\" }) # Word embeddings model Embeddings ({ \"path\" : vectors , \"storevectors\" : True , \"scoring\" : \"bm25\" , \"pca\" : 3 , \"quantize\" : True })","title":"Embeddings"},{"location":"embeddings/#configuration","text":"","title":"Configuration"},{"location":"embeddings/#method","text":"method : transformers|words Sets the sentence embeddings method to use. When set to transformers , the embeddings object builds sentence embeddings using the sentence transformers. Otherwise a word embeddings model is used. Defaults to words.","title":"method"},{"location":"embeddings/#path","text":"path : string Required field that sets the path for a vectors model. When method set to transformers , this must be a path to a Hugging Face transformers model. Otherwise, it must be a path to a local word embeddings model.","title":"path"},{"location":"embeddings/#tokenize","text":"tokenize : boolean Enables string tokenization (defaults to true). This method applies tokenization rules that work best with English language text and help increase the quality of English language sentence embeddings. This should be disabled when working with non-English text.","title":"tokenize"},{"location":"embeddings/#storevectors","text":"storevectors : boolean Enables copying of a vectors model set in path into the embeddings models output directory on save. This option enables a fully encapsulated index with no external file dependencies.","title":"storevectors"},{"location":"embeddings/#scoring","text":"scoring : bm25|tfidf|sif For word embedding models, a scoring model allows building weighted averages of word vectors for a given sentence. Supports BM25, tf-idf and SIF (smooth inverse frequency) methods. If a scoring method is not provided, mean sentence embeddings are built.","title":"scoring"},{"location":"embeddings/#pca","text":"pca : int Removes n principal components from generated sentence embeddings. When enabled, a TruncatedSVD model is built to help with dimensionality reduction. After pooling of vectors creates a single sentence embedding, this method is applied.","title":"pca"},{"location":"embeddings/#backend","text":"backend : annoy|faiss|hnsw Approximate Nearest Neighbor (ANN) index backend for storing generated sentence embeddings. Defaults to Faiss for Linux/macOS and Annoy for Windows. Faiss currently is not supported on Windows. Backend-specific settings are set with a corresponding configuration object having the same name as the backend (i.e. annoy, faiss, or hnsw). None of these are required and are set to defaults if omitted.","title":"backend"},{"location":"embeddings/#annoy","text":"annoy : ntrees : number of trees (int) - defaults to 10 searchk : search_k search setting (int) - defaults to -1 See Annoy documentation for more information on these parameters.","title":"annoy"},{"location":"embeddings/#faiss","text":"faiss : components : Comma separated list of components - defaults to None nprobe : search probe setting (int) - defaults to 6 See Faiss documentation on the index factory and search for more information on these parameters.","title":"faiss"},{"location":"embeddings/#hnsw","text":"hnsw : efconstruction : ef_construction param for init_index (int) - defaults to 200 m : M param for init_index (int) - defaults to 16 randomseed : random-seed param for init_index (init) - defaults to 100 efsearch : ef search param (int) - defaults to None and not set See Hnswlib documentation for more information on these parameters.","title":"hnsw"},{"location":"embeddings/#quantize","text":"quantize : boolean Enables quanitization of generated sentence embeddings. If the index backend supports it, sentence embeddings will be stored with 8-bit precision vs 32-bit. Only Faiss currently supports quantization.","title":"quantize"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.__init__","text":"Creates a new Embeddings model. Parameters: Name Type Description Default config embeddings configuration None Source code in txtai/embeddings/base.py def __init__ ( self , config = None ): \"\"\" Creates a new Embeddings model. Args: config: embeddings configuration \"\"\" # Configuration self . config = config # Embeddings model self . embeddings = None # Dimensionality reduction model self . lsa = None # Embedding scoring method - weighs each word in a sentence self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) if self . config and self . config . get ( \"scoring\" ) else None # Sentence vectors model self . model = self . loadVectors () if self . config else None","title":"__init__()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.batchsearch","text":"Finds documents in the embeddings model most similar to the input queries. Returns a list of (id, score) sorted by highest score per query, where id is the document id in the embeddings model. Parameters: Name Type Description Default queries queries text|tokens required limit maximum results 3 Returns: Type Description list of (id, score) per query Source code in txtai/embeddings/base.py def batchsearch ( self , queries , limit = 3 ): \"\"\" Finds documents in the embeddings model most similar to the input queries. Returns a list of (id, score) sorted by highest score per query, where id is the document id in the embeddings model. Args: queries: queries text|tokens limit: maximum results Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors embeddings = np . array ([ self . transform (( None , query , None )) for query in queries ]) # Search embeddings index results = self . embeddings . search ( embeddings , limit ) # Map ids if id mapping available lookup = self . config . get ( \"ids\" ) if lookup : results = [[( lookup [ i ], score ) for i , score in r ] for r in results ] return results","title":"batchsearch()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.batchsimilarity","text":"Computes the similarity between list of queries and list of text. Returns a list of (id, score) sorted by highest score per query, where id is the index in texts. Parameters: Name Type Description Default queries queries text|tokens required texts list of text|tokens required Returns: Type Description list of (id, score) per query Source code in txtai/embeddings/base.py def batchsimilarity ( self , queries , texts ): \"\"\" Computes the similarity between list of queries and list of text. Returns a list of (id, score) sorted by highest score per query, where id is the index in texts. Args: queries: queries text|tokens texts: list of text|tokens Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors queries = np . array ([ self . transform (( None , query , None )) for query in queries ]) texts = np . array ([ self . transform (( None , text , None )) for text in texts ]) # Dot product on normalized vectors is equal to cosine similarity scores = np . dot ( queries , texts . T ) . tolist () # Add index id and sort desc based on score return [ sorted ( enumerate ( score ), key = lambda x : x [ 1 ], reverse = True ) for score in scores ]","title":"batchsimilarity()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.batchtransform","text":"Transforms documents into embeddings vectors. Document text will be tokenized if not pre-tokenized. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Returns: Type Description embeddings vectors Source code in txtai/embeddings/base.py def batchtransform ( self , documents ): \"\"\" Transforms documents into embeddings vectors. Document text will be tokenized if not pre-tokenized. Args: documents: list of (id, text|tokens, tags) Returns: embeddings vectors \"\"\" return [ self . transform ( document ) for document in documents ]","title":"batchtransform()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.index","text":"Builds an embeddings index. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Source code in txtai/embeddings/base.py def index ( self , documents ): \"\"\" Builds an embeddings index. Args: documents: list of (id, text|tokens, tags) \"\"\" # Transform documents to embeddings vectors ids , dimensions , stream = self . model . index ( documents ) # Load streamed embeddings back to memory embeddings = np . empty (( len ( ids ), dimensions ), dtype = np . float32 ) with open ( stream , \"rb\" ) as queue : for x in range ( embeddings . shape [ 0 ]): embeddings [ x ] = pickle . load ( queue ) # Remove temporary file os . remove ( stream ) # Build LSA model (if enabled). Remove principal components from embeddings. if self . config . get ( \"pca\" ): self . lsa = self . buildLSA ( embeddings , self . config [ \"pca\" ]) self . removePC ( embeddings ) # Normalize embeddings self . normalize ( embeddings ) # Save embeddings metadata self . config [ \"ids\" ] = ids self . config [ \"dimensions\" ] = dimensions # Create embeddings index self . embeddings = ANNFactory . create ( self . config ) # Build the index self . embeddings . index ( embeddings )","title":"index()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.load","text":"Loads a pre-trained model. Models have the following files: config - configuration embeddings - sentence embeddings index lsa - LSA model, used to remove the principal component(s) scoring - scoring model used to weigh word vectors vectors - vectors model Parameters: Name Type Description Default path input directory path required Source code in txtai/embeddings/base.py def load ( self , path ): \"\"\" Loads a pre-trained model. Models have the following files: config - configuration embeddings - sentence embeddings index lsa - LSA model, used to remove the principal component(s) scoring - scoring model used to weigh word vectors vectors - vectors model Args: path: input directory path \"\"\" # Index configuration with open ( \" %s /config\" % path , \"rb\" ) as handle : self . config = pickle . load ( handle ) # Build full path to embedding vectors file if self . config . get ( \"storevectors\" ): self . config [ \"path\" ] = os . path . join ( path , self . config [ \"path\" ]) # Sentence embeddings index self . embeddings = ANNFactory . create ( self . config ) self . embeddings . load ( \" %s /embeddings\" % path ) # Dimensionality reduction if self . config . get ( \"pca\" ): with open ( \" %s /lsa\" % path , \"rb\" ) as handle : self . lsa = pickle . load ( handle ) # Embedding scoring if self . config . get ( \"scoring\" ): self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) self . scoring . load ( path ) # Sentence vectors model - transforms text into sentence embeddings self . model = self . loadVectors ()","title":"load()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.save","text":"Saves a model. Parameters: Name Type Description Default path output directory path required Source code in txtai/embeddings/base.py def save ( self , path ): \"\"\" Saves a model. Args: path: output directory path \"\"\" if self . config : # Create output directory, if necessary os . makedirs ( path , exist_ok = True ) # Copy vectors file if self . config . get ( \"storevectors\" ): shutil . copyfile ( self . config [ \"path\" ], os . path . join ( path , os . path . basename ( self . config [ \"path\" ]))) self . config [ \"path\" ] = os . path . basename ( self . config [ \"path\" ]) # Write index configuration with open ( \" %s /config\" % path , \"wb\" ) as handle : pickle . dump ( self . config , handle , protocol = pickle . HIGHEST_PROTOCOL ) # Write sentence embeddings index self . embeddings . save ( \" %s /embeddings\" % path ) # Save dimensionality reduction if self . lsa : with open ( \" %s /lsa\" % path , \"wb\" ) as handle : pickle . dump ( self . lsa , handle , protocol = pickle . HIGHEST_PROTOCOL ) # Save embedding scoring if self . scoring : self . scoring . save ( path )","title":"save()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.score","text":"Builds a scoring index. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Source code in txtai/embeddings/base.py def score ( self , documents ): \"\"\" Builds a scoring index. Args: documents: list of (id, text|tokens, tags) \"\"\" if self . scoring : # Build scoring index over documents self . scoring . index ( documents )","title":"score()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.search","text":"Finds documents in the embeddings model most similar to the input query. Returns a list of (id, score) sorted by highest score, where id is the document id in the embeddings model. Parameters: Name Type Description Default query query text|tokens required limit maximum results 3 Returns: Type Description list of (id, score) Source code in txtai/embeddings/base.py def search ( self , query , limit = 3 ): \"\"\" Finds documents in the embeddings model most similar to the input query. Returns a list of (id, score) sorted by highest score, where id is the document id in the embeddings model. Args: query: query text|tokens limit: maximum results Returns: list of (id, score) \"\"\" return self . batchsearch ([ query ], limit )[ 0 ]","title":"search()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.similarity","text":"Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. Parameters: Name Type Description Default query query text|tokens required texts list of text|tokens required Returns: Type Description list of (id, score) Source code in txtai/embeddings/base.py def similarity ( self , query , texts ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. Args: query: query text|tokens texts: list of text|tokens Returns: list of (id, score) \"\"\" return self . batchsimilarity ([ query ], texts )[ 0 ]","title":"similarity()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.transform","text":"Transforms document into an embeddings vector. Document text will be tokenized if not pre-tokenized. Parameters: Name Type Description Default document (id, text|tokens, tags) required Returns: Type Description embeddings vector Source code in txtai/embeddings/base.py def transform ( self , document ): \"\"\" Transforms document into an embeddings vector. Document text will be tokenized if not pre-tokenized. Args: document: (id, text|tokens, tags) Returns: embeddings vector \"\"\" # Convert document into sentence embedding embedding = self . model . transform ( document ) # Reduce the dimensionality of the embeddings. Scale the embeddings using this # model to reduce the noise of common but less relevant terms. if self . lsa : self . removePC ( embedding ) # Normalize embeddings self . normalize ( embedding ) return embedding","title":"transform()"},{"location":"examples/","text":"Examples The examples directory has a series of notebooks and applications giving an overview of txtai. See the sections below. Notebooks Notebook Description Introducing txtai Overview of the functionality provided by txtai Build an Embeddings index with Hugging Face Datasets Index and search Hugging Face Datasets Build an Embeddings index from a data source Index and search a data source with word embeddings Add semantic search to Elasticsearch Add semantic search to existing search systems Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling API Gallery Using txtai in JavaScript, Java, Rust and Go Building abstractive text summaries Run abstractive text summarization Extract text from documents Extract text from PDF, Office, HTML and more Transcribe audio to text Convert audio files to text Translate text between languages Streamline machine translation and language detection Similarity search with images Embed images and text into the same space for search Run pipeline workflows Simple yet powerful constructs to efficiently process data Applications Application Description Demo query shell Example application for search and indexing Image search Streamlit image search application","title":"Examples"},{"location":"examples/#examples","text":"The examples directory has a series of notebooks and applications giving an overview of txtai. See the sections below.","title":"Examples"},{"location":"examples/#notebooks","text":"Notebook Description Introducing txtai Overview of the functionality provided by txtai Build an Embeddings index with Hugging Face Datasets Index and search Hugging Face Datasets Build an Embeddings index from a data source Index and search a data source with word embeddings Add semantic search to Elasticsearch Add semantic search to existing search systems Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling API Gallery Using txtai in JavaScript, Java, Rust and Go Building abstractive text summaries Run abstractive text summarization Extract text from documents Extract text from PDF, Office, HTML and more Transcribe audio to text Convert audio files to text Translate text between languages Streamline machine translation and language detection Similarity search with images Embed images and text into the same space for search Run pipeline workflows Simple yet powerful constructs to efficiently process data","title":"Notebooks"},{"location":"examples/#applications","text":"Application Description Demo query shell Example application for search and indexing Image search Streamlit image search application","title":"Applications"},{"location":"install/","text":"Installation The easiest way to install is via pip and PyPI pip install txtai You can also install txtai directly from GitHub. Using a Python Virtual Environment is recommended. pip install git+https://github.com/neuml/txtai Python 3.6+ is supported. txtai has the following environment specific prerequisites. Linux Optional audio transcription requires a system library to be installed macOS Run brew install libomp see this link Windows Install C++ Build Tools","title":"Installation"},{"location":"install/#installation","text":"The easiest way to install is via pip and PyPI pip install txtai You can also install txtai directly from GitHub. Using a Python Virtual Environment is recommended. pip install git+https://github.com/neuml/txtai Python 3.6+ is supported. txtai has the following environment specific prerequisites.","title":"Installation"},{"location":"install/#linux","text":"Optional audio transcription requires a system library to be installed","title":"Linux"},{"location":"install/#macos","text":"Run brew install libomp see this link","title":"macOS"},{"location":"install/#windows","text":"Install C++ Build Tools","title":"Windows"},{"location":"workflows/","text":"Workflows Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows don't know they are working with pipelines but enable efficient processing of pipeline data. Workflows are streaming by nature and work on data in batches, allowing large volumes of data to be processed efficiently. An example Workflow is shown below. This workflow will work with both documents and audio files. Documents will have text extracted and summarized. Audio files will be transcribed. Both results will be joined, translated into French and loaded into an Embeddings index. # file:// prefixes are required to signal to the workflow this is a file and not a text string files = [ \"file://txtai/article.pdf\" , \"file://txtai/US_tops_5_million.wav\" , \"file://txtai/Canadas_last_fully.wav\" , \"file://txtai/Beijing_mobilises.wav\" , \"file://txtai/The_National_Park.wav\" , \"file://txtai/Maine_man_wins_1_mil.wav\" , \"file://txtai/Make_huge_profits.wav\" ] data = [( x , element , None ) for x , element in enumerate ( files )] # Workflow that extracts text and builds a summary articles = Workflow ([ FileTask ( textractor ), Task ( lambda x : summary ([ y [: 1024 ] for y in x ])) ]) # Define workflow tasks. Workflows can also be tasks! tasks = [ WorkflowTask ( articles , r \".\\.pdf$\" ), FileTask ( transcribe , r \"\\.wav$\" ), Task ( lambda x : translate ( x , \"fr\" )), Task ( index , unpack = False ) ] # Workflow that translates text to French workflow = Workflow ( tasks ) for _ in workflow ( data ): pass __init__ ( self , tasks , batch = 100 ) special Creates a new workflow. Workflows are lists of tasks to execute. Parameters: Name Type Description Default tasks list of workflow tasks required batch how many items to process at a time, defaults to 100 100 Source code in txtai/workflow/base.py def __init__ ( self , tasks , batch = 100 ): \"\"\" Creates a new workflow. Workflows are lists of tasks to execute. Args: tasks: list of workflow tasks batch: how many items to process at a time, defaults to 100 \"\"\" self . tasks = tasks self . batch = batch __call__ ( self , elements ) special Executes a workflow for input elements. Parameters: Name Type Description Default elements list of data elements required Returns: Type Description transformed data elements Source code in txtai/workflow/base.py def __call__ ( self , elements ): \"\"\" Executes a workflow for input elements. Args: elements: list of data elements Returns: transformed data elements \"\"\" batch = [] for x in elements : batch . append ( x ) if len ( batch ) == self . batch : yield from self . process ( batch ) if batch : yield from self . process ( batch ) Tasks __init__ ( self , action = None , select = None , unpack = True ) special Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Actions are callable functions. Parameters: Name Type Description Default action action to execute on each data element None select filter(s) used to select data to process None unpack if data elements should be unpacked or unwrapped from (id, data, tag) tuples True Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Actions are callable functions. Args: action: action to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples \"\"\" self . action = action self . select = select self . unpack = unpack File Task Task that processes file urls Image Task Task that processes image urls Workflow Task Task that runs a Workflow. Allows creating workflows of workflows.","title":"Workflows"},{"location":"workflows/#workflows","text":"Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows don't know they are working with pipelines but enable efficient processing of pipeline data. Workflows are streaming by nature and work on data in batches, allowing large volumes of data to be processed efficiently. An example Workflow is shown below. This workflow will work with both documents and audio files. Documents will have text extracted and summarized. Audio files will be transcribed. Both results will be joined, translated into French and loaded into an Embeddings index. # file:// prefixes are required to signal to the workflow this is a file and not a text string files = [ \"file://txtai/article.pdf\" , \"file://txtai/US_tops_5_million.wav\" , \"file://txtai/Canadas_last_fully.wav\" , \"file://txtai/Beijing_mobilises.wav\" , \"file://txtai/The_National_Park.wav\" , \"file://txtai/Maine_man_wins_1_mil.wav\" , \"file://txtai/Make_huge_profits.wav\" ] data = [( x , element , None ) for x , element in enumerate ( files )] # Workflow that extracts text and builds a summary articles = Workflow ([ FileTask ( textractor ), Task ( lambda x : summary ([ y [: 1024 ] for y in x ])) ]) # Define workflow tasks. Workflows can also be tasks! tasks = [ WorkflowTask ( articles , r \".\\.pdf$\" ), FileTask ( transcribe , r \"\\.wav$\" ), Task ( lambda x : translate ( x , \"fr\" )), Task ( index , unpack = False ) ] # Workflow that translates text to French workflow = Workflow ( tasks ) for _ in workflow ( data ): pass","title":"Workflows"},{"location":"workflows/#txtai.workflow.base.Workflow.__init__","text":"Creates a new workflow. Workflows are lists of tasks to execute. Parameters: Name Type Description Default tasks list of workflow tasks required batch how many items to process at a time, defaults to 100 100 Source code in txtai/workflow/base.py def __init__ ( self , tasks , batch = 100 ): \"\"\" Creates a new workflow. Workflows are lists of tasks to execute. Args: tasks: list of workflow tasks batch: how many items to process at a time, defaults to 100 \"\"\" self . tasks = tasks self . batch = batch","title":"__init__()"},{"location":"workflows/#txtai.workflow.base.Workflow.__call__","text":"Executes a workflow for input elements. Parameters: Name Type Description Default elements list of data elements required Returns: Type Description transformed data elements Source code in txtai/workflow/base.py def __call__ ( self , elements ): \"\"\" Executes a workflow for input elements. Args: elements: list of data elements Returns: transformed data elements \"\"\" batch = [] for x in elements : batch . append ( x ) if len ( batch ) == self . batch : yield from self . process ( batch ) if batch : yield from self . process ( batch )","title":"__call__()"},{"location":"workflows/#tasks","text":"","title":"Tasks"},{"location":"workflows/#txtai.workflow.task.base.Task.__init__","text":"Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Actions are callable functions. Parameters: Name Type Description Default action action to execute on each data element None select filter(s) used to select data to process None unpack if data elements should be unpacked or unwrapped from (id, data, tag) tuples True Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Actions are callable functions. Args: action: action to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples \"\"\" self . action = action self . select = select self . unpack = unpack","title":"__init__()"},{"location":"workflows/#file-task","text":"Task that processes file urls","title":"File Task"},{"location":"workflows/#image-task","text":"Task that processes image urls","title":"Image Task"},{"location":"workflows/#workflow-task","text":"Task that runs a Workflow. Allows creating workflows of workflows.","title":"Workflow Task"},{"location":"pipelines/extractor/","text":"Extractor An Extractor pipeline is a combination of an embeddings query and an Extractive QA model. Filtering the context for a QA model helps maximize performance of the model. Extractor parameters are set as constructor arguments. Examples below. Extractor ( embeddings , path , quantize , gpu , model , tokenizer ) __init__ ( self , embeddings , path , quantize = False , gpu = False , model = None , tokenizer = None ) special Builds a new extractor. Parameters: Name Type Description Default embeddings embeddings model required path path to qa model required quantize True if model should be quantized before inference, False otherwise. False gpu if gpu inference should be used (only works if GPUs are available) False model optional existing pipeline model to wrap None tokenizer Tokenizer class None Source code in txtai/pipeline/extractor.py def __init__ ( self , embeddings , path , quantize = False , gpu = False , model = None , tokenizer = None ): \"\"\" Builds a new extractor. Args: embeddings: embeddings model path: path to qa model quantize: True if model should be quantized before inference, False otherwise. gpu: if gpu inference should be used (only works if GPUs are available) model: optional existing pipeline model to wrap tokenizer: Tokenizer class \"\"\" # Embeddings model self . embeddings = embeddings # QA Pipeline self . pipeline = Questions ( path , quantize , gpu , model ) # Tokenizer class use default method if not set self . tokenizer = tokenizer if tokenizer else Tokenizer __call__ ( self , queue , texts ) special Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Parameters: Name Type Description Default queue input queue (name, query, question, snippet) required texts list of text required Returns: Type Description list of (name, answer) Source code in txtai/pipeline/extractor.py def __call__ ( self , queue , texts ): \"\"\" Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Args: queue: input queue (name, query, question, snippet) texts: list of text Returns: list of (name, answer) \"\"\" # Execute embeddings query results = self . query ([ query for _ , query , _ , _ in queue ], texts ) # Build question-context pairs names , questions , contexts , snippets = [], [], [], [] for x , ( name , _ , question , snippet ) in enumerate ( queue ): # Build context using top n best matching segments topn = sorted ( results [ x ], key = lambda y : y [ 2 ], reverse = True )[: 3 ] context = \" \" . join ([ text for _ , text , _ in sorted ( topn , key = lambda y : y [ 0 ])]) names . append ( name ) questions . append ( question ) contexts . append ( context ) snippets . append ( snippet ) # Run qa pipeline and return answers return self . answers ( names , questions , contexts , snippets )","title":"Extractor"},{"location":"pipelines/extractor/#extractor","text":"An Extractor pipeline is a combination of an embeddings query and an Extractive QA model. Filtering the context for a QA model helps maximize performance of the model. Extractor parameters are set as constructor arguments. Examples below. Extractor ( embeddings , path , quantize , gpu , model , tokenizer )","title":"Extractor"},{"location":"pipelines/extractor/#txtai.pipeline.extractor.Extractor.__init__","text":"Builds a new extractor. Parameters: Name Type Description Default embeddings embeddings model required path path to qa model required quantize True if model should be quantized before inference, False otherwise. False gpu if gpu inference should be used (only works if GPUs are available) False model optional existing pipeline model to wrap None tokenizer Tokenizer class None Source code in txtai/pipeline/extractor.py def __init__ ( self , embeddings , path , quantize = False , gpu = False , model = None , tokenizer = None ): \"\"\" Builds a new extractor. Args: embeddings: embeddings model path: path to qa model quantize: True if model should be quantized before inference, False otherwise. gpu: if gpu inference should be used (only works if GPUs are available) model: optional existing pipeline model to wrap tokenizer: Tokenizer class \"\"\" # Embeddings model self . embeddings = embeddings # QA Pipeline self . pipeline = Questions ( path , quantize , gpu , model ) # Tokenizer class use default method if not set self . tokenizer = tokenizer if tokenizer else Tokenizer","title":"__init__()"},{"location":"pipelines/extractor/#txtai.pipeline.extractor.Extractor.__call__","text":"Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Parameters: Name Type Description Default queue input queue (name, query, question, snippet) required texts list of text required Returns: Type Description list of (name, answer) Source code in txtai/pipeline/extractor.py def __call__ ( self , queue , texts ): \"\"\" Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Args: queue: input queue (name, query, question, snippet) texts: list of text Returns: list of (name, answer) \"\"\" # Execute embeddings query results = self . query ([ query for _ , query , _ , _ in queue ], texts ) # Build question-context pairs names , questions , contexts , snippets = [], [], [], [] for x , ( name , _ , question , snippet ) in enumerate ( queue ): # Build context using top n best matching segments topn = sorted ( results [ x ], key = lambda y : y [ 2 ], reverse = True )[: 3 ] context = \" \" . join ([ text for _ , text , _ in sorted ( topn , key = lambda y : y [ 0 ])]) names . append ( name ) questions . append ( question ) contexts . append ( context ) snippets . append ( snippet ) # Run qa pipeline and return answers return self . answers ( names , questions , contexts , snippets )","title":"__call__()"},{"location":"pipelines/labels/","text":"Labels A Labels pipeline uses a zero shot classification model to apply labels to input text. Labels parameters are set as constructor arguments. Examples below. Labels () Labels ( \"roberta-large-mnli\" ) __init__ ( self , path = None , quantize = False , gpu = True , model = None ) special Source code in txtai/pipeline/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"zero-shot-classification\" , path , quantize , gpu , model ) __call__ ( self , text , labels , multilabel = False ) special Applies a zero shot classifier to text using a list of labels. Returns a list of (id, score) sorted by highest score, where id is the index in labels. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default text text|list required labels list of labels required Returns: Type Description list of (id, score) Source code in txtai/pipeline/labels.py def __call__ ( self , text , labels , multilabel = False ): \"\"\" Applies a zero shot classifier to text using a list of labels. Returns a list of (id, score) sorted by highest score, where id is the index in labels. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: text: text|list labels: list of labels Returns: list of (id, score) \"\"\" # Run ZSL pipeline results = self . pipeline ( text , labels , multi_label = multilabel ) # Convert results to a list if necessary if not isinstance ( results , list ): results = [ results ] # Build list of (id, score) scores = [] for result in results : scores . append ([( labels . index ( label ), result [ \"scores\" ][ x ]) for x , label in enumerate ( result [ \"labels\" ])]) return scores [ 0 ] if isinstance ( text , str ) else scores","title":"Labels"},{"location":"pipelines/labels/#labels","text":"A Labels pipeline uses a zero shot classification model to apply labels to input text. Labels parameters are set as constructor arguments. Examples below. Labels () Labels ( \"roberta-large-mnli\" )","title":"Labels"},{"location":"pipelines/labels/#txtai.pipeline.labels.Labels.__init__","text":"Source code in txtai/pipeline/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"zero-shot-classification\" , path , quantize , gpu , model )","title":"__init__()"},{"location":"pipelines/labels/#txtai.pipeline.labels.Labels.__call__","text":"Applies a zero shot classifier to text using a list of labels. Returns a list of (id, score) sorted by highest score, where id is the index in labels. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default text text|list required labels list of labels required Returns: Type Description list of (id, score) Source code in txtai/pipeline/labels.py def __call__ ( self , text , labels , multilabel = False ): \"\"\" Applies a zero shot classifier to text using a list of labels. Returns a list of (id, score) sorted by highest score, where id is the index in labels. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: text: text|list labels: list of labels Returns: list of (id, score) \"\"\" # Run ZSL pipeline results = self . pipeline ( text , labels , multi_label = multilabel ) # Convert results to a list if necessary if not isinstance ( results , list ): results = [ results ] # Build list of (id, score) scores = [] for result in results : scores . append ([( labels . index ( label ), result [ \"scores\" ][ x ]) for x , label in enumerate ( result [ \"labels\" ])]) return scores [ 0 ] if isinstance ( text , str ) else scores","title":"__call__()"},{"location":"pipelines/overview/","text":"Pipelines txtai provides a generic pipeline processing framework with the only interface requirement being a __call__ method. Pipelines are flexible and can process various types of data. Pipelines can wrap machine learning models as well as other processes. The following pipeline types are currently supported. Hugging Face pipelines Extractive QA Labeling Similarity Summary Hugging Face models Transcription Translation Data processing calls Text extraction","title":"Overview"},{"location":"pipelines/overview/#pipelines","text":"txtai provides a generic pipeline processing framework with the only interface requirement being a __call__ method. Pipelines are flexible and can process various types of data. Pipelines can wrap machine learning models as well as other processes. The following pipeline types are currently supported. Hugging Face pipelines Extractive QA Labeling Similarity Summary Hugging Face models Transcription Translation Data processing calls Text extraction","title":"Pipelines"},{"location":"pipelines/similarity/","text":"Similarity A Similarity pipeline is also a zero shot classifier model where the labels are the queries. The results are transposed to get scores per query/label vs scores per input text. Similarity parameters are set as constructor arguments. Examples below. Similarity () Similarity ( \"roberta-large-mnli\" ) __init__ ( self , path = None , quantize = False , gpu = True , model = None ) special Source code in txtai/pipeline/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"zero-shot-classification\" , path , quantize , gpu , model ) __call__ ( self , query , texts , multilabel = True ) special Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default query query text|list required texts list of text required Returns: Type Description list of (id, score) Source code in txtai/pipeline/similarity.py def __call__ ( self , query , texts , multilabel = True ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: query: query text|list texts: list of text Returns: list of (id, score) \"\"\" # Call Labels pipeline for texts using input query as the candidate label scores = super () . __call__ ( texts , [ query ] if isinstance ( query , str ) else query , multilabel ) # Sort on query index id scores = [[ score for _ , score in sorted ( row )] for row in scores ] # Transpose axes to get a list of text scores for each query scores = np . array ( scores ) . T . tolist () # Build list of (id, score) per query sorted by highest score scores = [ sorted ( enumerate ( row ), key = lambda x : x [ 1 ], reverse = True ) for row in scores ] return scores [ 0 ] if isinstance ( query , str ) else scores","title":"Similarity"},{"location":"pipelines/similarity/#similarity","text":"A Similarity pipeline is also a zero shot classifier model where the labels are the queries. The results are transposed to get scores per query/label vs scores per input text. Similarity parameters are set as constructor arguments. Examples below. Similarity () Similarity ( \"roberta-large-mnli\" )","title":"Similarity"},{"location":"pipelines/similarity/#txtai.pipeline.labels.Similarity.__init__","text":"Source code in txtai/pipeline/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"zero-shot-classification\" , path , quantize , gpu , model )","title":"__init__()"},{"location":"pipelines/similarity/#txtai.pipeline.similarity.Similarity.__call__","text":"Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default query query text|list required texts list of text required Returns: Type Description list of (id, score) Source code in txtai/pipeline/similarity.py def __call__ ( self , query , texts , multilabel = True ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: query: query text|list texts: list of text Returns: list of (id, score) \"\"\" # Call Labels pipeline for texts using input query as the candidate label scores = super () . __call__ ( texts , [ query ] if isinstance ( query , str ) else query , multilabel ) # Sort on query index id scores = [[ score for _ , score in sorted ( row )] for row in scores ] # Transpose axes to get a list of text scores for each query scores = np . array ( scores ) . T . tolist () # Build list of (id, score) per query sorted by highest score scores = [ sorted ( enumerate ( row ), key = lambda x : x [ 1 ], reverse = True ) for row in scores ] return scores [ 0 ] if isinstance ( query , str ) else scores","title":"__call__()"},{"location":"pipelines/summary/","text":"Summary A Summary pipeline summarizes text. Summary parameters are set as constructor arguments. Examples below. Summary () Summary ( \"sshleifer/distilbart-cnn-12-3\" ) __init__ ( self , path = None , quantize = False , gpu = True , model = None ) special Source code in txtai/pipeline/summary.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"summarization\" , path , quantize , gpu , model ) __call__ ( self , text , minlength = None , maxlength = None ) special Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Parameters: Name Type Description Default text text|list required minlength minimum length for summary None maxlength maximum length for summary None Returns: Type Description summary text Source code in txtai/pipeline/summary.py def __call__ ( self , text , minlength = None , maxlength = None ): \"\"\" Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Args: text: text|list minlength: minimum length for summary maxlength: maximum length for summary Returns: summary text \"\"\" kwargs = { \"truncation\" : True } if minlength : kwargs [ \"min_length\" ] = minlength if maxlength : kwargs [ \"max_length\" ] = maxlength # Run summarization pipeline results = self . pipeline ( text , ** kwargs ) # Convert results to a list if necessary if not isinstance ( results , list ): results = [ results ] # Pull out summary text results = [ self . clean ( x [ \"summary_text\" ]) for x in results ] return results [ 0 ] if isinstance ( text , str ) else results","title":"Summary"},{"location":"pipelines/summary/#summary","text":"A Summary pipeline summarizes text. Summary parameters are set as constructor arguments. Examples below. Summary () Summary ( \"sshleifer/distilbart-cnn-12-3\" )","title":"Summary"},{"location":"pipelines/summary/#txtai.pipeline.summary.Summary.__init__","text":"Source code in txtai/pipeline/summary.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"summarization\" , path , quantize , gpu , model )","title":"__init__()"},{"location":"pipelines/summary/#txtai.pipeline.summary.Summary.__call__","text":"Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Parameters: Name Type Description Default text text|list required minlength minimum length for summary None maxlength maximum length for summary None Returns: Type Description summary text Source code in txtai/pipeline/summary.py def __call__ ( self , text , minlength = None , maxlength = None ): \"\"\" Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Args: text: text|list minlength: minimum length for summary maxlength: maximum length for summary Returns: summary text \"\"\" kwargs = { \"truncation\" : True } if minlength : kwargs [ \"min_length\" ] = minlength if maxlength : kwargs [ \"max_length\" ] = maxlength # Run summarization pipeline results = self . pipeline ( text , ** kwargs ) # Convert results to a list if necessary if not isinstance ( results , list ): results = [ results ] # Pull out summary text results = [ self . clean ( x [ \"summary_text\" ]) for x in results ] return results [ 0 ] if isinstance ( text , str ) else results","title":"__call__()"},{"location":"pipelines/textractor/","text":"Textractor A Textractor pipeline extracts and splits text from documents. Textractor parameters are set as constructor arguments. Examples below. Textractor () __init__ ( self , sentences = False , paragraphs = False , minlength = None , join = False ) special Creates a new Textractor. Parameters: Name Type Description Default sentences tokenize text into sentences if True, defaults to False False paragraphs tokenizes text into paragraphs if True, defaults to False False minlength require at least minlength characters per text element, defaults to None None join joins tokenized sections back together if True, defaults to False False Source code in txtai/pipeline/textractor.py def __init__ ( self , sentences = False , paragraphs = False , minlength = None , join = False ): \"\"\" Creates a new Textractor. Args: sentences: tokenize text into sentences if True, defaults to False paragraphs: tokenizes text into paragraphs if True, defaults to False minlength: require at least minlength characters per text element, defaults to None join: joins tokenized sections back together if True, defaults to False \"\"\" self . sentences = sentences self . paragraphs = paragraphs self . minlength = minlength self . join = join __call__ ( self , files ) special Extracts text from a file at path. This method supports files as a string or a list. If the input is a string, the return type is text|list for the file. If files is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Parameters: Name Type Description Default files text|list required Returns: Type Description extracted text from files Source code in txtai/pipeline/textractor.py def __call__ ( self , files ): \"\"\" Extracts text from a file at path. This method supports files as a string or a list. If the input is a string, the return type is text|list for the file. If files is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: files: text|list Returns: extracted text from files \"\"\" # Get inputs values = [ files ] if not isinstance ( files , list ) else files # Extract text for each input file results = [] for path in values : parsed = parser . from_file ( path ) text = parsed [ \"content\" ] if text : # Parse and add extracted results results . append ( self . parse ( text )) return results [ 0 ] if isinstance ( files , str ) else results","title":"Textractor"},{"location":"pipelines/textractor/#textractor","text":"A Textractor pipeline extracts and splits text from documents. Textractor parameters are set as constructor arguments. Examples below. Textractor ()","title":"Textractor"},{"location":"pipelines/textractor/#txtai.pipeline.textractor.Textractor.__init__","text":"Creates a new Textractor. Parameters: Name Type Description Default sentences tokenize text into sentences if True, defaults to False False paragraphs tokenizes text into paragraphs if True, defaults to False False minlength require at least minlength characters per text element, defaults to None None join joins tokenized sections back together if True, defaults to False False Source code in txtai/pipeline/textractor.py def __init__ ( self , sentences = False , paragraphs = False , minlength = None , join = False ): \"\"\" Creates a new Textractor. Args: sentences: tokenize text into sentences if True, defaults to False paragraphs: tokenizes text into paragraphs if True, defaults to False minlength: require at least minlength characters per text element, defaults to None join: joins tokenized sections back together if True, defaults to False \"\"\" self . sentences = sentences self . paragraphs = paragraphs self . minlength = minlength self . join = join","title":"__init__()"},{"location":"pipelines/textractor/#txtai.pipeline.textractor.Textractor.__call__","text":"Extracts text from a file at path. This method supports files as a string or a list. If the input is a string, the return type is text|list for the file. If files is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Parameters: Name Type Description Default files text|list required Returns: Type Description extracted text from files Source code in txtai/pipeline/textractor.py def __call__ ( self , files ): \"\"\" Extracts text from a file at path. This method supports files as a string or a list. If the input is a string, the return type is text|list for the file. If files is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: files: text|list Returns: extracted text from files \"\"\" # Get inputs values = [ files ] if not isinstance ( files , list ) else files # Extract text for each input file results = [] for path in values : parsed = parser . from_file ( path ) text = parsed [ \"content\" ] if text : # Parse and add extracted results results . append ( self . parse ( text )) return results [ 0 ] if isinstance ( files , str ) else results","title":"__call__()"},{"location":"pipelines/transcription/","text":"Transcription A Transcription pipeline converts speech in audio files to text. Transcription parameters are set as constructor arguments. Examples below. Transcription () Transcription ( \"facebook/wav2vec2-large-960h\" ) __init__ ( self , path = 'facebook/wav2vec2-base-960h' , quantize = False , gpu = True , batch = 64 ) special Constructs a new transcription pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/wav2vec2-base-960h' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 Source code in txtai/pipeline/transcription.py def __init__ ( self , path = \"facebook/wav2vec2-base-960h\" , quantize = False , gpu = True , batch = 64 ): \"\"\" Constructs a new transcription pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) if not SOUNDFILE : raise ImportError ( \"SoundFile library not installed or libsndfile not found\" ) # load model and processor self . model = Wav2Vec2ForCTC . from_pretrained ( self . path ) self . processor = Wav2Vec2Processor . from_pretrained ( self . path ) # Move model to device self . model . to ( self . device ) __call__ ( self , files ) special Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default files text|list required Returns: Type Description list of transcribed text Source code in txtai/pipeline/transcription.py def __call__ ( self , files ): \"\"\" Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: files: text|list Returns: list of transcribed text \"\"\" values = [ files ] if not isinstance ( files , list ) else files # Parse audio files speech = [ sf . read ( f ) for f in values ] # Get unique list of sampling rates unique = set ( s [ 1 ] for s in speech ) results = {} for sampling in unique : # Get inputs for current sampling rate inputs = [( x , s [ 0 ]) for x , s in enumerate ( speech ) if s [ 1 ] == sampling ] # Transcribe in batches outputs = [] for chunk in self . batch ([ s for _ , s in inputs ], self . batchsize ): outputs . extend ( self . transcribe ( chunk , sampling )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] . capitalize () # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( files , str ) else results","title":"Transcription"},{"location":"pipelines/transcription/#transcription","text":"A Transcription pipeline converts speech in audio files to text. Transcription parameters are set as constructor arguments. Examples below. Transcription () Transcription ( \"facebook/wav2vec2-large-960h\" )","title":"Transcription"},{"location":"pipelines/transcription/#txtai.pipeline.transcription.Transcription.__init__","text":"Constructs a new transcription pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/wav2vec2-base-960h' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 Source code in txtai/pipeline/transcription.py def __init__ ( self , path = \"facebook/wav2vec2-base-960h\" , quantize = False , gpu = True , batch = 64 ): \"\"\" Constructs a new transcription pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) if not SOUNDFILE : raise ImportError ( \"SoundFile library not installed or libsndfile not found\" ) # load model and processor self . model = Wav2Vec2ForCTC . from_pretrained ( self . path ) self . processor = Wav2Vec2Processor . from_pretrained ( self . path ) # Move model to device self . model . to ( self . device )","title":"__init__()"},{"location":"pipelines/transcription/#txtai.pipeline.transcription.Transcription.__call__","text":"Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default files text|list required Returns: Type Description list of transcribed text Source code in txtai/pipeline/transcription.py def __call__ ( self , files ): \"\"\" Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: files: text|list Returns: list of transcribed text \"\"\" values = [ files ] if not isinstance ( files , list ) else files # Parse audio files speech = [ sf . read ( f ) for f in values ] # Get unique list of sampling rates unique = set ( s [ 1 ] for s in speech ) results = {} for sampling in unique : # Get inputs for current sampling rate inputs = [( x , s [ 0 ]) for x , s in enumerate ( speech ) if s [ 1 ] == sampling ] # Transcribe in batches outputs = [] for chunk in self . batch ([ s for _ , s in inputs ], self . batchsize ): outputs . extend ( self . transcribe ( chunk , sampling )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] . capitalize () # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( files , str ) else results","title":"__call__()"},{"location":"pipelines/translation/","text":"Translation A Translation pipeline translates text between languages Translation parameters are set as constructor arguments. Examples below. Translation () __init__ ( self , path = 'facebook/m2m100_418M' , quantize = False , gpu = True , batch = 64 , langdetect = 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz' ) special Constructs a new language translation pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/m2m100_418M' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 langdetect path to language detection model, uses a default path if not provided 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz' Source code in txtai/pipeline/translation.py def __init__ ( self , path = \"facebook/m2m100_418M\" , quantize = False , gpu = True , batch = 64 , langdetect = DEFAULT_LANG_DETECT ): \"\"\" Constructs a new language translation pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content langdetect: path to language detection model, uses a default path if not provided \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) # Language detection self . detector = None self . langdetect = langdetect # Language models self . models = {} self . ids = self . available () __call__ ( self , texts , target = 'en' , source = None ) special Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default texts text|list required target target language code, defaults to \"en\" 'en' source source language code, detects language if not provided None Returns: Type Description list of translated text Source code in txtai/pipeline/translation.py def __call__ ( self , texts , target = \"en\" , source = None ): \"\"\" Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: texts: text|list target: target language code, defaults to \"en\" source: source language code, detects language if not provided Returns: list of translated text \"\"\" values = [ texts ] if not isinstance ( texts , list ) else texts # Detect source languages languages = self . detect ( values ) if not source else [ source ] * len ( values ) unique = set ( languages ) # Build list of (index, language, text) values = [( x , lang , values [ x ]) for x , lang in enumerate ( languages )] results = {} for language in unique : # Get all text values for language inputs = [( x , text ) for x , lang , text in values if lang == language ] # Translate text in batches outputs = [] for chunk in self . batch ([ text for _ , text in inputs ], self . batchsize ): outputs . extend ( self . translate ( chunk , language , target )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( texts , str ) else results","title":"Translation"},{"location":"pipelines/translation/#translation","text":"A Translation pipeline translates text between languages Translation parameters are set as constructor arguments. Examples below. Translation ()","title":"Translation"},{"location":"pipelines/translation/#txtai.pipeline.translation.Translation.__init__","text":"Constructs a new language translation pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/m2m100_418M' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 langdetect path to language detection model, uses a default path if not provided 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz' Source code in txtai/pipeline/translation.py def __init__ ( self , path = \"facebook/m2m100_418M\" , quantize = False , gpu = True , batch = 64 , langdetect = DEFAULT_LANG_DETECT ): \"\"\" Constructs a new language translation pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content langdetect: path to language detection model, uses a default path if not provided \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) # Language detection self . detector = None self . langdetect = langdetect # Language models self . models = {} self . ids = self . available ()","title":"__init__()"},{"location":"pipelines/translation/#txtai.pipeline.translation.Translation.__call__","text":"Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default texts text|list required target target language code, defaults to \"en\" 'en' source source language code, detects language if not provided None Returns: Type Description list of translated text Source code in txtai/pipeline/translation.py def __call__ ( self , texts , target = \"en\" , source = None ): \"\"\" Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: texts: text|list target: target language code, defaults to \"en\" source: source language code, detects language if not provided Returns: list of translated text \"\"\" values = [ texts ] if not isinstance ( texts , list ) else texts # Detect source languages languages = self . detect ( values ) if not source else [ source ] * len ( values ) unique = set ( languages ) # Build list of (index, language, text) values = [( x , lang , values [ x ]) for x , lang in enumerate ( languages )] results = {} for language in unique : # Get all text values for language inputs = [( x , text ) for x , lang , text in values if lang == language ] # Translate text in batches outputs = [] for chunk in self . batch ([ text for _ , text in inputs ], self . batchsize ): outputs . extend ( self . translate ( chunk , language , target )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( texts , str ) else results","title":"__call__()"}]}